# NGINX

- It act as a Reverse proxy
- It provide load balancing
- It can provide static content and is much faster than node.js in so
- It provides ssl termination
- It can be used as a web Server
- It provide huge concurrent connection handling mechanism
- It improve scalability and performance

---
## ğŸ”· What is the **Master Process** in Nginx?

The **master process** is the **parent process** of Nginx.

### ğŸ”‘ Responsibilities of Master Process

* Reads and **validates `nginx.conf`**
* **Starts** worker processes
* **Manages** workers (restart, shutdown)
* Handles **reloads** (`nginx -s reload`)
* Runs with **higher privileges** (root)

ğŸ“Œ **It does NOT handle client requests**

---

## ğŸ”· What is a **Worker Process** in Nginx?

Worker processes are **child processes** created by the master.

### ğŸ”‘ Responsibilities of Worker Processes

* Handle **all client requests**
* Manage **connections**
* Perform **I/O operations**
* Serve HTTP/TCP traffic

ğŸ“Œ Workers run as a **non-root user** (security)

---

## ğŸ§  How They Work Together

1. Master starts
2. Master reads config
3. Master spawns workers
4. Workers handle traffic
5. Master monitors workers

---

## ğŸ”¢ How Many Workers?

Defined in **main context**:

```nginx
worker_processes auto;
```

* `auto` â†’ equals number of CPU cores
* Best practice for performance

---

## ğŸ”— Workers + Events (Important Link)

```nginx
events {
    worker_connections 1024;
}
```

### Max connections:

```text
max_connections = worker_processes Ã— worker_connections
```

---

## ğŸ” View Master & Workers (macOS)

```bash
ps aux | grep nginx
```

Output:

```text
root     nginx: master process /opt/homebrew/bin/nginx
nobody   nginx: worker process
nobody   nginx: worker process
```

---

## ğŸ›¡ï¸ Why This Design? (Big Advantage)

* **High performance**
* **Non-blocking I/O**
* **Fault tolerant** (worker crash â‰  downtime)
* **Hot reload** without stopping server

---

## ğŸ§  Simple Analogy (Interview Friendly)

| Nginx   | Real World |
| ------- | ---------- |
| Master  | Manager    |
| Worker  | Employees  |
| Clients | Customers  |

Manager manages, employees work.

---

## ğŸ¯ Interview One-Liner (Perfect)

> â€œNginx uses a masterâ€“worker architecture where the master manages configuration and lifecycle, and worker processes handle all client requests using an event-driven model.â€

---
## Basic Settings

- brew install nginx
<pre>
  nginx                # start
  nginx -s stop        # stop
  nginx -s reload      # reload config
  nginx -t             # test config
</pre>

- configuration file is located at ` /opt/homebrew/etc/nginx/nginx.conf`


## Core NGINX structure
<pre>
 main context
    â”œâ”€â”€ events
    â””â”€â”€ http
      â”œâ”€â”€ upstream
      â””â”€â”€ server
           â””â”€â”€ location

  </pre>

### Main context
<pre>
The main context:
     Exists outside of events {}, http {}, server {}, etc.
     Applies globally to the entire Nginx process
     Affects master & worker processes

     Used for global settings like:
            user
            worker_processes
            error_log
            pid 
</pre>


---

# ğŸ§  What is `events` Context in NGINX?

> The `events` context controls **how NGINX handles low-level connections** between clients and NGINX workers.

It has **nothing to do with routing, APIs, proxying, or backend logic**.

---

# ğŸ— Where `events` Fits in NGINX

NGINX has **two layers**:

```
OS-level connection handling  â†’  events {}
HTTP-level request handling  â†’  http {}
```

So:

* `events {}` â†’ **TCP connections**
* `http {}` â†’ **HTTP requests**

Thatâ€™s the key distinction.

---

# ğŸ” Why `events` Exists

NGINX is famous because it:

* handles **100k+ concurrent connections**
* uses **event-driven, non-blocking I/O**

The `events` block tells NGINX:

* how many connections each worker can handle
* which OS event system to use (epoll / kqueue)

---

# ğŸ§© What Goes Inside `events {}`

In **99% of backend work**, youâ€™ll see just this:

```nginx
events {
    worker_connections 1024;
}
```

Thatâ€™s it.

---

# ğŸ”‘ Most Important Directive: `worker_connections`

### What it means:

> Maximum number of simultaneous connections **per worker process**

Example:

```nginx
worker_processes 4;
worker_connections 1024;
```

Total connections â‰ˆ

```
4 Ã— 1024 = ~4096 connections
```

---

# ğŸ§  Memory Hook (VERY IMPORTANT)

> **events = connections, not requests**

Requests live in `http {}`
Connections live in `events {}`

Once you remember this, confusion ends.

---

# ğŸ§ª Do Backend Engineers Tune `events`?

### Real answer:

âŒ Almost never

### Why?

* Defaults are already good
* Cloud load balancers sit in front
* Horizontal scaling is preferred

Only infra / SRE teams tune this heavily.

---


# ğŸ§± Minimal Safe Template (USE THIS ALWAYS)

```nginx
events {
    worker_connections 1024;
}

http {
    server {
        listen 80;

        location / {
            proxy_pass http://localhost:3000;
        }
    }
}
```

This is **100% acceptable** in interviews and real projects.

---

# âš ï¸ Common Mistake (DO NOT DO THIS)

âŒ Trying to put:

```nginx
proxy_pass
location
server
```

inside `events {}`

ğŸ‘‰ Thatâ€™s an instant red flag.

---

# ğŸ§  One-Line Mental Summary (WRITE THIS DOWN)

> **events = connection handling**
> **http = request handling**

---


## Treasure Chest 
---

## âœ… Correct Mental Model (Simple & Clean)

### ğŸ”¹ **Main context**

ğŸ‘‰ **Controls Nginx as a whole**

* Global settings
* Process management
* Worker creation
* Security (`user`)
* Logging

ğŸ“Œ *Does NOT handle traffic directly*

---

### ğŸ”¹ **Events block**

ğŸ‘‰ **Controls how connections are handled**

* How many connections
* Event-driven mechanism
* Low-level networking behavior

ğŸ“Œ *Connection handling, not request logic*

---

### ğŸ”¹ **HTTP block**

ğŸ‘‰ **Handles HTTP requests**

* Websites
* APIs
* Routing
* Reverse proxy
* Load balancing (HTTP)

ğŸ“Œ *This is where real web logic lives*

---

## ğŸ§  One-Line Summary (Perfect)

> **Main manages Nginx itself, events manage connections, and http manages requests.**

---

## ğŸ”— Putting It Together (Visual)

```
Main (Nginx control)
 â”œâ”€â”€ Events (Connections)
 â””â”€â”€ HTTP (Requests)
      â””â”€â”€ Server (Virtual hosts)
           â””â”€â”€ Location (URL handling)
```

---

## â— Small but Important Note

* **Connections â‰  Requests**
* A single connection can handle **multiple HTTP requests** (keep-alive)

---


## Protocols supported by NGINX are :-

Nginx officially supports **three main protocol contexts**:

## 1ï¸âƒ£ `http`

* For **web traffic** (HTTP/HTTPS)
* Handles:

  * Websites
  * REST APIs
  * Reverse proxy & load balancing
* Uses blocks like `server` and `location`

```nginx
http {
    server {
        listen 80;
    }
}
```

---

## 2ï¸âƒ£ `stream`

* For **TCP/UDP traffic**
* Handles:

  * MySQL, Redis, SSH, MQTT, any TCP/UDP service
* Uses blocks like `server` inside `stream`

```nginx
stream {
    server {
        listen 3306;
        proxy_pass 127.0.0.1:3306;
    }
}
```

---

## 3ï¸âƒ£ `mail`

* For **email protocols**
* Handles:

  * SMTP, IMAP, POP3
* Less commonly used

```nginx
mail {
    server {
        listen 25;
        protocol smtp;
    }
}
```

---

### âŒ Notes:

* You **cannot define arbitrary protocols**; only these three are officially supported.
* `main` and `events` are **not protocols**, they are contexts for configuration.
* Each protocol has its **own top-level block**, just like `http`.

---

### ğŸ”— Summary Table

| Context | Handles                          |
| ------- | -------------------------------- |
| main    | Global Nginx settings            |
| events  | Connections & worker behavior    |
| http    | HTTP/HTTPS requests              |
| stream  | TCP/UDP traffic                  |
| mail    | Email protocols (SMTP/IMAP/POP3) |

---
##  Nginx Directives

## 1ï¸âƒ£ `proxy_pass`

### **What it does:**

It tells NGINX **where to forward the request** â€” basically your **backend server**.

### **Use case:**

You have a Node.js API running on `localhost:3000`.
Client requests come to NGINX at `/api`. You want NGINX to forward all `/api` requests to Node.js.

```nginx
location /api {
    proxy_pass http://localhost:3000;
}
```

**Mental hook:**

> â€œClient talks to NGINX, NGINX talks to backend.â€
> **proxy_pass = â€˜backend addressâ€™**

---

## 2ï¸âƒ£ `proxy_set_header`

### **What it does:**

Adds or overwrites HTTP headers when NGINX forwards a request.

### **Use case:**

* Node.js wants **real client IP**, not NGINXâ€™s IP.
* Node.js wants **host info** to route requests correctly.

```nginx
proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
```

**Memory hook:**

> â€œWhen proxying, NGINX can fake or forward headers. These three headers are must-have.â€

* `$host` â†’ domain requested
* `$remote_addr` â†’ client IP
* `$proxy_add_x_forwarded_for` â†’ preserves chain of IPs

ğŸ’¡ **Think:** Every time your backend logs IP â†’ this is why it works.

---

## 3ï¸âƒ£ `proxy_http_version`

### **What it does:**

Sets which HTTP version NGINX uses to talk to the backend.

```nginx
proxy_http_version 1.1;
```

**Use case:**

* WebSocket or chunked transfer requires **HTTP/1.1**.
* Default is **1.0**, which can break modern APIs.

**Memory hook:**

> â€œIf you use proxy_pass + WebSocket â†’ must tell NGINX: use HTTP/1.1â€

---

## 4ï¸âƒ£ `proxy_read_timeout`

### **What it does:**

Max time NGINX waits **for the backend to send a response** after connection is established.

```nginx
proxy_read_timeout 30s;
```

**Use case:**

* Node.js API takes 25 seconds to respond â†’ default timeout may cut it off.
* Set `proxy_read_timeout` longer than your slowest expected API.

**Memory hook:**

> â€œHow long do I wait for the answer? â†’ proxy_read_timeoutâ€

---

## 5ï¸âƒ£ `proxy_connect_timeout`

### **What it does:**

Max time NGINX waits **to establish connection with backend**.

```nginx
proxy_connect_timeout 5s;
```

**Use case:**

* Backend is down or unreachable â†’ timeout after 5 seconds instead of hanging forever.
* Protects NGINX workers from getting stuck.

**Memory hook:**

> â€œHow long to knock on backend door? â†’ proxy_connect_timeoutâ€

---

## 6ï¸âƒ£ `proxy_send_timeout`

### **What it does:**

Max time NGINX waits **to send the request to the backend**.

```nginx
proxy_send_timeout 10s;
```

**Use case:**

* Backend is slow to read incoming request â†’ NGINX aborts after timeout.
* Protects your server in case backend is overloaded.

**Memory hook:**

> â€œHow long do I take to hand over the request? â†’ proxy_send_timeoutâ€

---

## âœ… Quick Mental Map to Remember

| Directive               | Think â€œWhat question does it answer?â€             |
| ----------------------- | ------------------------------------------------- |
| `proxy_pass`            | Where do I send this request?                     |
| `proxy_set_header`      | What info should backend know about client?       |
| `proxy_http_version`    | Which HTTP version should I speak to backend?     |
| `proxy_read_timeout`    | How long will I wait for backendâ€™s answer?        |
| `proxy_connect_timeout` | How long do I try to connect to backend?          |
| `proxy_send_timeout`    | How long will I spend sending request to backend? |

---

### ğŸ§  Memory Trick (Sticky)

**Sentence:**

> â€œI **pass** the request (`proxy_pass`) with the **right headers** (`proxy_set_header`) using HTTP/1.1 (`proxy_http_version`), wait some time to **connect** (`proxy_connect_timeout`), **send** the request (`proxy_send_timeout`), and finally **wait for response** (`proxy_read_timeout`).â€

---






