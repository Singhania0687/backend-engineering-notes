# Kafka

## 1ï¸âƒ£ Why Kafka exists (problem it solves)

### Traditional backend problem

Imagine:

* User places an order
* You need to:

  * Save order
  * Send email
  * Send SMS
  * Update inventory
  * Update analytics
  * Notify delivery service

### Without Kafka (tight coupling)

```
Order Service â†’ Email Service
             â†’ SMS Service
             â†’ Inventory Service
             â†’ Analytics Service
```

âŒ Problems:

* If **any service is down**, order fails
* Hard to scale
* High latency
* Tight coupling
* Difficult retries

---

### With Kafka (event-driven)

```
Order Service â†’ Kafka â†’ Email Service
                        SMS Service
                        Inventory Service
                        Analytics Service
```

âœ… Benefits:

* Loose coupling
* High throughput
* Fault tolerance
* Async processing
* Easy to add new consumers

ðŸ‘‰ **Kafka is a distributed event streaming platform**

---

## 2ï¸âƒ£ What Kafka actually is 

> Kafka is a **distributed commit log** that stores events in order and lets multiple systems read them independently.

Key words:

* **Distributed**
* **Persistent**
* **Ordered**
* **Scalable**
* **Fault-tolerant**

---

## 3ï¸âƒ£ Core Kafka components 

### 1. Producer

* Sends messages (events) to Kafka
* Example: Order Service

### 2. Consumer

* Reads messages from Kafka
* Example: Email Service

### 3. Topic

* Logical stream of messages
* Example:

  * `order_created`
  * `payment_completed`

Think of **topic = table**, messages = rows

---

### 4. Partition (most important concept)

Each topic is split into **partitions**.

```
Topic: order_created
Partition 0 â†’ msg1 â†’ msg2 â†’ msg3
Partition 1 â†’ msg4 â†’ msg5
Partition 2 â†’ msg6
```

Why partitions?

* Parallelism
* Scalability
* High throughput

ðŸ‘‰ **Order is guaranteed only within a partition**

---

### 5. Broker

* A Kafka server
* Stores partitions

Cluster example:

```
Broker 1 â†’ Partition 0
Broker 2 â†’ Partition 1
Broker 3 â†’ Partition 2
```

---

### 6. Offset

* Each message has an offset (like index)

```
Partition 0:
offset 0 â†’ msg
offset 1 â†’ msg
offset 2 â†’ msg
```

Consumers track offsets to know **where they left off**

---

## 4ï¸âƒ£ Consumer Groups (INTERVIEW FAVORITE)

### What is a consumer group?

* A group of consumers sharing load

Rules:

* **One partition â†’ one consumer in a group**
* Multiple groups can read the same topic independently

```
Topic: order_created (3 partitions)

Consumer Group A:
  C1 â†’ P0
  C2 â†’ P1
  C3 â†’ P2

Consumer Group B:
  C4 â†’ P0
  C5 â†’ P1
  C6 â†’ P2
```

ðŸ‘‰ Used for:

* Horizontal scaling
* Independent services

---

## 5ï¸âƒ£ Message delivery semantics

Kafka supports:

### 1. At-most-once

* No retry
* Fast
* Possible data loss

### 2. At-least-once (most common)

* Retry on failure
* Possible duplicates

### 3. Exactly-once (advanced)

* No loss
* No duplicates
* Needs idempotent producers + transactions

ðŸ‘‰ Interview answer:

> Kafka guarantees **at-least-once by default**

---

## 6ï¸âƒ£ Kafka storage model 

### Kafka writes are:

* **Append-only**
* Sequential disk writes

Disk structure:

```
log-0
log-1
log-2
```

Why fast?

* No random writes
* OS page cache
* Zero-copy (sendfile)

ðŸ‘‰ Kafka is disk-based but still extremely fast

---

## 7ï¸âƒ£ Retention & durability

Kafka does **not delete messages after consumption**

Retention policies:

* Time-based (e.g., 7 days)
* Size-based (e.g., 100GB)

Meaning:

* You can replay events
* New consumers can read old data

---

## 8ï¸âƒ£ Replication & fault tolerance

Each partition has:

* Leader
* Followers

```
Partition 0:
Leader â†’ Broker 1
Follower â†’ Broker 2
Follower â†’ Broker 3
```

If leader dies:

* Follower becomes leader
* No data loss (if replication factor â‰¥ 2)

---

## 9ï¸âƒ£ ZooKeeper vs KRaft (important)

### Old Kafka

* Used ZooKeeper for:

  * Metadata
  * Leader election

### New Kafka (modern)

* Uses **KRaft mode**
* No ZooKeeper dependency

ðŸ‘‰ Interview:

> Kafka is moving away from ZooKeeper to KRaft for simplicity and performance

---

## ðŸ”Ÿ Kafka vs traditional message queues

| Feature         | Kafka           | RabbitMQ    |
| --------------- | --------------- | ----------- |
| Throughput      | Very high       | Medium      |
| Persistence     | Yes             | Optional    |
| Replay messages | Yes             | No          |
| Ordering        | Per partition   | Per queue   |
| Use case        | Event streaming | Task queues |

---

## 1ï¸âƒ£1ï¸âƒ£ Real-world Kafka use cases (MAANG level)

* Order events
* Payment pipelines
* Clickstream analytics
* Log aggregation
* CDC (Change Data Capture)
* Real-time dashboards
* Microservice communication

Amazon / Netflix / Uber:

* Kafka for **event backbone**
* Services donâ€™t talk directly

---

## 1ï¸âƒ£2ï¸âƒ£ Kafka in microservices architecture

```
Service A â†’ Kafka Topic â†’ Service B
                         Service C
                         Service D
```

Benefits:

* Services deployed independently
* Failure isolation
* Async processing

---

## 1ï¸âƒ£3ï¸âƒ£ Common interview questions 

### Q1. Why Kafka is fast?

* Sequential disk writes
* Zero-copy
* Partitioning
* Batch processing

### Q2. Ordering guarantee?

* Only within a partition

### Q3. What happens if consumer crashes?

* Offset not committed
* Another consumer resumes

### Q4. Difference between Kafka and SQS?

* Kafka = streaming + replay
* SQS = queue, message removed after consume

### Q5. Can Kafka replace database?

âŒ No
Kafka is **not for querying**, itâ€™s for streaming

---

## 1ï¸âƒ£ Fix the mental model (MongoDB â†’ Kafka mapping)

Think like this:

| MongoDB          | Kafka           |
| ---------------- | --------------- |
| Database         | Kafka Cluster   |
| Collection       | Topic           |
| Document         | Event / Message |
| Insert document  | Produce event   |
| Find / Read      | Consume event   |
| Index            | Partition       |
| Cursor position  | Offset          |
| Multiple readers | Consumer Group  |

ðŸ‘‰ **Key difference**:
MongoDB = *pull data when needed*
Kafka = *events are pushed continuously & replayable*

---

## 2ï¸âƒ£ Where does Kafka actually live?

Kafka is **not a library inside your app**.

Kafka is:

* A **separate distributed system**
* Runs as **Kafka brokers** (processes)
* Stores data on **disk**, not memory

Typical setup:

```
Kafka Cluster
 â”œâ”€â”€ Broker 1
 â”œâ”€â”€ Broker 2
 â””â”€â”€ Broker 3
```

Each broker:

* Runs on a machine / VM / container
* Stores **topic partitions as files on disk**

---

## 3ï¸âƒ£ What is a Topic REALLY?

A **topic** is just a **named log**.

Example:

```
Topic: user-signup
```

Internally:

```
user-signup
 â”œâ”€â”€ Partition 0  (file on disk)
 â”œâ”€â”€ Partition 1  (file on disk)
 â””â”€â”€ Partition 2  (file on disk)
```

Each partition is:

* **Append-only**
* Stored as **segment files on disk**
* Ordered **only within that partition**

ðŸ‘‰ **Events are NEVER updated or deleted**
(only expired via retention policy)

---

## 4ï¸âƒ£ Where are events stored?

**On Kafka broker disks**

Example:

```
/var/lib/kafka/
 â””â”€â”€ user-signup-0/
      â”œâ”€â”€ 000000000000000.log
      â”œâ”€â”€ 000000000000123.log
      â””â”€â”€ ...
```

Each `.log` file contains:

```
[offset][timestamp][key][value]
```

So:

* Kafka is basically a **distributed commit log**
* Not a queue that â€œremovesâ€ messages after consumption

---

## 5ï¸âƒ£ Who creates WHAT and WHERE?

### âœ… Topic creation

Topics are created:

* **Once**
* By **infra / DevOps / platform team**
* Or automatically (bad practice in prod)

Ways to create topic:

```
kafka-topics.sh --create ...
```

or

```
Admin API
```

You **do NOT create topics inside business logic** usually.

---

### âœ… Partitions

Partitions are:

* Decided **at topic creation time**
* Define **parallelism**
* Define **ordering guarantee**

Rule:

```
More partitions = more parallel consumers
```

You donâ€™t dynamically create partitions per event.

---

### âŒ Partition group

There is **NO such thing** as a "partition group".

You probably mean:

* **Consumer Group**

---

## 6ï¸âƒ£ What is a Consumer Group (very important)

A **consumer group** is:

* A **logical name**
* Used to coordinate multiple consumers

Example:

```
Consumer Group: email-service
```

Rules:

1. One partition â†’ **only ONE consumer in a group**
2. Multiple groups can read same topic independently

```
Topic: user-signup (3 partitions)

Group: email-service
 â”œâ”€â”€ Consumer A â†’ Partition 0
 â”œâ”€â”€ Consumer B â†’ Partition 1
 â””â”€â”€ Consumer C â†’ Partition 2

Group: analytics-service
 â”œâ”€â”€ Consumer X â†’ Partition 0
 â”œâ”€â”€ Consumer Y â†’ Partition 1
 â””â”€â”€ Consumer Z â†’ Partition 2
```

ðŸ‘‰ Each group gets **its own offset tracking**

---

## 7ï¸âƒ£ Where are offsets stored?

Offsets are stored:

* **Inside Kafka itself**
* In a special topic:

```
__consumer_offsets
```

So Kafka tracks:

```
(group, topic, partition) â†’ last_committed_offset
```

This is why consumers can:

* Crash
* Restart
* Resume from exact position

---

## 8ï¸âƒ£ Producer flow (step-by-step)

When a service produces an event:

```
Service
  |
  | 1. Serialize event
  | 2. Choose topic
  | 3. Choose partition (key-based or round-robin)
  |
Kafka Broker
  |
  | 4. Append to partition file
  | 5. Replicate to other brokers
```

Important:

* Producer **does NOT know consumers**
* Producer **only cares about topic**

---

## 9ï¸âƒ£ Consumer flow (step-by-step)

When a service consumes:

```
Consumer
  |
  | 1. Join consumer group
  | 2. Kafka assigns partitions
  | 3. Consumer polls data
  | 4. Processes event
  | 5. Commits offset
```

ðŸ‘‰ Kafka never â€œpushesâ€ messages
Consumers **pull** via polling

---

## ðŸ”Ÿ Compare again with MongoDB (clear picture)

### MongoDB

```
Service A â†’ insert into collection
Service B â†’ query collection
Data may be deleted/updated
```

### Kafka

```
Service A â†’ append event to topic
Service B â†’ reads event stream
Service C â†’ reads same stream independently
Data is immutable
```

Kafka = **event log**
MongoDB = **state store**

---

## 1ï¸âƒ£1ï¸âƒ£ When should YOU create what?

### As an application developer:

You usually:

* Produce to existing topics
* Consume from existing topics
* Define consumer group name

### Infra / Platform team:

* Create topics
* Decide partitions
* Set retention, replication

---

## 1ï¸âƒ£2ï¸âƒ£ One sentence that fixes everything

> **Kafka stores events as immutable, append-only logs on disk, partitioned for parallelism, and consumer groups independently track how much of the log they have read.**

---

# 1ï¸âƒ£ What exactly is a **Kafka Broker**?

> **A broker is just a Kafka server process running on a machine.**

More concretely:

* A **broker = JVM process**
* Runs Kafka code
* Has **disk**
* Listens on a port (like `9092`)
* Stores topic **partitions as files**

Example:

```
Kafka Cluster
 â”œâ”€â”€ Broker 1 (machine A)
 â”œâ”€â”€ Broker 2 (machine B)
 â””â”€â”€ Broker 3 (machine C)
```

Each broker:

* Stores **some partitions**
* Handles **produce & consume requests**
* Replicates data to other brokers

ðŸ‘‰ Broker â‰  topic
ðŸ‘‰ Broker â‰  consumer
ðŸ‘‰ Broker â‰  service

---


> â€œWe have Kafka cluster â†’ inside that we have topics â†’ inside topics we have eventsâ€

Now we just **insert two missing layers**:

```
Kafka Cluster
 â”œâ”€â”€ Broker 1
 â”œâ”€â”€ Broker 2
 â””â”€â”€ Broker 3

Topic: order-created
 â”œâ”€â”€ Partition 0
 â”œâ”€â”€ Partition 1
 â””â”€â”€ Partition 2

Partition
 â””â”€â”€ Events (ordered, immutable)
```

---

# 3ï¸âƒ£ Where do **partitions** fit in?

Partitions solve **2 problems**:

1. **Scalability**
2. **Parallel consumption**

### Without partitions:

* Only one consumer can read at a time
* Throughput is limited

### With partitions:

* Each partition can be processed independently

Think of a topic as:

> **A folder, partitions are files inside it**

---

## Real analogy

```
Topic = Book
Partitions = Chapters
Events = Pages
Offsets = Page numbers
```

---

# 4ï¸âƒ£ What is an **offset**?

Offset is:

> **The position of an event inside a partition**

Example:

```
Partition 0:
offset 0 â†’ event A
offset 1 â†’ event B
offset 2 â†’ event C
```

Important:

* Offsets are **per-partition**
* Offset is **never reused**
* Offset â‰  global across topic

So:

```
Topic has no offset
Partition has offsets
```

---
## What a consumer group REALLY is

> **A consumer group is NOT a group of services.
> It is a group of CONSUMER INSTANCES reading together.**

Key rules:

1. Consumer group = **string name**
2. Consumers with same group â†’ **cooperate**
3. Each partition â†’ **only one consumer per group**

---

## Example with your 8 services and 5 topics

Assume:

* 8 microservices
* Each service can consume multiple topics
* Each service instance can join multiple consumer groups

### YES â€” a service CAN belong to multiple consumer groups

But **not automatically** â€” it depends on code.

Example inside one service:

```js
consumerA â†’ group = email-service
consumerB â†’ group = analytics-service
```

Same service, two different groups.

---

# 6ï¸âƒ£ Consumer group â‰  AWS IAM group (important difference)

AWS group:

* Permission grouping
* Static
* Security-based

Kafka consumer group:

* **Load balancing mechanism**
* Runtime coordination
* Offset tracking

So:
âŒ Not permission based
âŒ Not organizational
âœ… Processing-based

---

# 7ï¸âƒ£ Visualize consumer groups correctly

### Topic: `user-created` (3 partitions)

```
Partition 0
Partition 1
Partition 2
```

### Group: `email-service`

```
EmailConsumer-1 â†’ Partition 0
EmailConsumer-2 â†’ Partition 1
EmailConsumer-3 â†’ Partition 2
```

### Group: `fraud-service`

```
FraudConsumer-1 â†’ Partition 0
FraudConsumer-2 â†’ Partition 1
FraudConsumer-3 â†’ Partition 2
```

Same data
Different offsets
Independent processing

---

# 8ï¸âƒ£ Who owns offsets?

Offsets belong to:

```
(consumer group + topic + partition)
```

Stored in Kafka:

```
__consumer_offsets topic
```

So Kafka knows:

```
email-service, user-created, partition 1 â†’ offset 42
```

---

# 9ï¸âƒ£ Where partitions live physically?

Partitions are **distributed across brokers**.

Example:

```
Broker 1 â†’ user-created-0
Broker 2 â†’ user-created-1
Broker 3 â†’ user-created-2
```

Plus replicas for fault tolerance.

So:

* Topic is logical
* Partition is physical
* Broker stores partitions

---

# ðŸ”Ÿ Final correct mental model (lock this)

```
Kafka Cluster
 â”œâ”€â”€ Brokers (machines/processes)
 â”‚    â””â”€â”€ Store partitions (files on disk)
 â”‚
 â””â”€â”€ Topics
      â””â”€â”€ Partitions
           â””â”€â”€ Events (with offsets)

Producers â†’ append to partitions
Consumers â†’ read partitions via consumer groups
Offsets â†’ track progress per group
```

---

# 1ï¸âƒ£1ï¸âƒ£ One line that should click now

> **Topics organize events, partitions scale them, brokers store them, consumer groups process them, and offsets remember progress.**


# âœ… First, confirm brokers 

> **Broker = running Kafka server process (instance) inside the cluster**

Now forget brokers for a moment.
Letâ€™s focus only on **topic â†’ partitions â†’ consumer groups**.

---

# 1ï¸âƒ£ What is a PARTITION 

> **A partition is just a numbered bucket inside a topic.**

Topic:

```
orders
```

With 3 partitions:

```
orders
 â”œâ”€â”€ bucket-0
 â”œâ”€â”€ bucket-1
 â””â”€â”€ bucket-2
```

Each bucket:

* Stores events **in order**
* Events are added **only at the end**
* Nothing is removed when read

---

## Why partitions exist (simple reason)

Without partitions:

* Only **1 consumer** can read at a time

With partitions:

* **Multiple consumers** can read **in parallel**

ðŸ‘‰ **Partitions = parallelism + scalability**

---

# 2ï¸âƒ£ What exactly is stored inside a partition?

Inside ONE partition:

```
offset 0 â†’ event A
offset 1 â†’ event B
offset 2 â†’ event C
offset 3 â†’ event D
```

Key rules:

* Offset starts from 0
* Offset increases forever
* Offset is **local to that partition**
* Order is guaranteed **only inside a partition**

---

# 3ï¸âƒ£ What is a CONSUMER GROUP (ultra-simple)

> **A consumer group is a team of consumers that SHARE the work.**

They read:

* Same topic
* Same partitions
* But **split the partitions among themselves**

---

## Simple rule (MOST IMPORTANT)

> **Inside one consumer group, one partition can be read by ONLY ONE consumer.**

Thatâ€™s it.
Everything else comes from this rule.

---

# 4ï¸âƒ£ Example (no jargon yet)

### Topic: `orders`

### Partitions: 3

```
Partition 0
Partition 1
Partition 2
```

### Consumer group: `payment-service`

Case 1ï¸âƒ£ â€” 1 consumer:

```
Consumer A â†’ P0, P1, P2
```

Case 2ï¸âƒ£ â€” 3 consumers:

```
Consumer A â†’ P0
Consumer B â†’ P1
Consumer C â†’ P2
```

Case 3ï¸âƒ£ â€” 5 consumers:

```
Consumer A â†’ P0
Consumer B â†’ P1
Consumer C â†’ P2
Consumer D â†’ idle
Consumer E â†’ idle
```

ðŸ‘‰ Consumers > partitions = **idle consumers**

---

# 5ï¸âƒ£ Why consumer groups exist (simple)

Consumer groups exist to answer **one question**:

> â€œDo we want multiple consumers to SHARE work or do SAME work?â€

---

## Case A: SHARE work (same group)

```
order-processing-service (group)
 â”œâ”€â”€ instance 1
 â”œâ”€â”€ instance 2
 â””â”€â”€ instance 3
```

Each event processed **once**

---

## Case B: SAME work (different groups)

```
email-service (group)
fraud-service (group)
analytics-service (group)
```

Each group gets **all events**

---

# 6ï¸âƒ£ Very important clarification (your confusion)

You asked:

> â€œWe group services under one and create such groups that is consumer group?â€

âŒ Not exactly

Correct understanding:

> **Consumer group groups CONSUMER INSTANCES, not services logically.**

Same service can:

* Join **multiple consumer groups**
* Consume **multiple topics**
* Have **multiple consumers inside it**

---

# 7ï¸âƒ£ Where do OFFSETS fit in? (simple)

> **Offset = â€œhow far a consumer group has readâ€**

Example:

```
Group: payment-service
Partition 1
Last committed offset = 42
```

Means:

* Events up to 42 are processed
* Next read starts from 43

Important:

* Offset is tracked **per group per partition**
* Different groups have different offsets

---

# 8ï¸âƒ£ Now connect ALL pieces (simple picture)

```
Topic: orders
 â”œâ”€â”€ Partition 0 (offsets 0,1,2...)
 â”œâ”€â”€ Partition 1 (offsets 0,1,2...)
 â””â”€â”€ Partition 2 (offsets 0,1,2...)

Consumer Group: payment-service
 â”œâ”€â”€ Consumer A â†’ Partition 0
 â”œâ”€â”€ Consumer B â†’ Partition 1
 â””â”€â”€ Consumer C â†’ Partition 2

Consumer Group: email-service
 â”œâ”€â”€ Consumer X â†’ Partition 0
 â”œâ”€â”€ Consumer Y â†’ Partition 1
 â””â”€â”€ Consumer Z â†’ Partition 2
```

Same data
Different offsets
Independent processing

---

# 9ï¸âƒ£ Now a LITTLE jargon 

| Simple word       | Kafka term     |
| ----------------- | -------------- |
| Bucket            | Partition      |
| Team of readers   | Consumer Group |
| Position          | Offset         |
| Event             | Record         |
| Read              | Poll           |
| Remember position | Commit offset  |

---

# ðŸ”Ÿ One sentence that should make it click forever

> **A topic is split into partitions so work can be parallelized, and consumer groups decide whether consumers share that work or all do it independently.**

---

# Importnace of Consumer Group
1.  **Multiple consumer instances are not for more concurrency than partitions â€” they are for reliability, scalability, and isolation.**

Concurrency is capped by partitions.
Instances solve *other problems*.

---

2. What creating multiple instances actually achieves

### âœ… 1. Parallel processing (up to partition count)

You already know this:

```
Partitions = 3
Instances = 3
â†’ true parallel consumption
```

But thatâ€™s **only the first benefit**.

---

### âœ… 2. Fault tolerance (VERY important)

If you have **only one instance**:

```
Instance A â†’ P0, P1, P2
```

If it crashes:

* Entire service stops
* No processing happens

---

With multiple instances:

```
Instance A â†’ P0
Instance B â†’ P1
Instance C â†’ P2
```

If **Instance B dies**:

* Kafka rebalances
* Another instance picks up P1
* Processing resumes

ðŸ‘‰ **No single point of failure**

---

### âœ… 3. Horizontal scalability (real-world reason)

Traffic grows:

* Events per second â†‘
* Processing cost per event â†‘

You can:

* Increase partitions (infra decision)
* Increase instances (app decision)

Instances give you **runtime flexibility**.

---

### âœ… 4. Deployment safety (rolling deployments)

In production:

```
Instance A (old version)
Instance B (old version)
Instance C (old version)
```

Deploy new version:

* Restart one instance at a time
* Others keep processing

ðŸ‘‰ **Zero downtime**

---

### âœ… 5. Load isolation inside the service

Even if:

* One instance is slow (GC pause, CPU spike)
* Others keep processing their partitions

Kafka **isolates impact** per partition.

---

### âœ… 6. Backpressure handling

If:

* One partition gets heavy data
* One instance is overloaded

Kafka wonâ€™t block the whole topic.
Only that partition lags.

---

## 3ï¸âƒ£ Important clarification

> **More instances than partitions does NOT increase throughput.**

```
Partitions = 3
Instances = 10
Active = 3
Idle = 7
```

Extra instances = **standby capacity**

---

## 4ï¸âƒ£ Why not just rely on partitions then?

Because partitions:

* Are fixed at topic level
* Expensive to change in prod
* Affect ordering guarantees

Instances:

* Cheap to add/remove
* App-level control
* No data reshuffling

---
### Partitions = lanes on a highway

### Consumer instances = cars

* More lanes â†’ more cars can move in parallel
* More cars than lanes â†’ some wait
* If one car breaks â†’ others keep moving

---

## 6ï¸âƒ£ One-line answer 

> â€œPartitions define the maximum parallelism, while multiple consumer instances provide fault tolerance, horizontal scalability, and operational safety without increasing partition count.â€

---

## 7ï¸âƒ£ Final takeaway (burn this in)

> **Partitions give capacity.
> Instances give resilience.**



